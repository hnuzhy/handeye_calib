{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCTag Handeye Calibration\n",
    "This notebook shows how to perform hand-eye calibration with CCTag pattern.\n",
    "\n",
    "Hand-eye calibration is an important task in computer vision and robotics, which aims to determine the spatial relationship between the camera and the robot's end effector. This process is crucial for achieving accurate grasping, positioning, and manipulation.\n",
    "\n",
    "The CCTag pattern consists of a set of concentric circles with different radii, forming a unique pattern. This design allows the tag to be recognized at different scales and viewing angles while maintaining high recognition accuracy. Each CCTag pattern has a central circle surrounded by several concentric rings, which encode information through the width, number and arrangement order of the rings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependency import and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "\n",
    "import rlia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `os` module is a part of the Python standard library that provides a way to interact with the operating system. It offers functions for navigating the file system, manipulating file paths, and managing environment variables.\n",
    "\n",
    "The `json` module is also a part of the Python standard library and is used for working with JSON (JavaScript Object Notation) data. It provides methods for serializing Python objects into JSON format and deserializing JSON data back into Python objects.\n",
    "\n",
    "The `pickle` module is a Python standard library that allows for the serialization and deserialization of Python objects. It is commonly used to save and load complex data structures, such as the calibration data we will be working with in this notebook.\n",
    "\n",
    "The `numpy` library is fundamental for numerical computations in Python. It provides support for arrays, matrices, and a collection of mathematical functions to operate on these data structures.\n",
    "\n",
    "The `matplotlib.pyplot` module is a collection of functions that make matplotlib work like MATLAB. It is used for creating static, animated, and interactive visualizations in Python.\n",
    "\n",
    "The `open3d` library is designed for processing 3D data. It supports 3D data visualization, point cloud processing, and mesh processing, making it essential for working with RGB-D data in robotic applications.\n",
    "\n",
    "The `open3d.core` module provides lower-level core functionality for Open3D, including tensor operations and low-level data structures. It is used for efficient processing of large-scale 3D data.\n",
    "\n",
    "The `rlia` library is a Robot Library for Indoor Applications. It includes various modules for data handling, calibration, and utility functions, which are vital for implementing the hand-eye calibration process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper Functions\n",
    "Code for visualization, transform rgbd data to point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_rgb_to_pc(depth_map:np.array, rgb:np.array, intrinsic:np.array) -> o3d.geometry.PointCloud:\n",
    "    \"\"\" Convert RGB-D images to a point cloud.  \n",
    "  \n",
    "    Args:  \n",
    "        depth_map (np.array): A 2D array representing depth values (Z) of the scene.  \n",
    "        rgb (np.array): A 2D or 3D array representing RGB color values of the scene.   \n",
    "                    If 2D, it is assumed to be a grayscale image that will be repeated to create an RGB image.  \n",
    "        intrinsic (np.array): A 3x3 intrinsic camera matrix.  \n",
    "  \n",
    "    Returns:  \n",
    "        o3d.geometry.PointCloud: A point cloud object containing 3D points and their corresponding RGB colors.  \n",
    "    \"\"\"  \n",
    "    # Get the resolution of the depth map  \n",
    "    resolution_x, resolution_y = depth_map.shape  \n",
    "      \n",
    "    # Flatten the depth map to a 1D array  \n",
    "    real_depth = depth_map.reshape(resolution_x * resolution_y)  \n",
    "  \n",
    "    # Check if the RGB image is in color or grayscale  \n",
    "    if (len(rgb.shape) == 3):  # Color image  \n",
    "        rgb_list = rgb.reshape(resolution_x * resolution_y, 3) / 255.0  # Normalize RGB values  \n",
    "    elif (len(rgb.shape) == 2):  # Grayscale image  \n",
    "        rgb_copy = np.repeat(rgb[:, :, None], 3, axis=2)  # Repeat grayscale values to create an RGB image  \n",
    "        rgb_list = rgb_copy.reshape(resolution_x * resolution_y, 3) / 255.0  # Normalize RGB values  \n",
    "  \n",
    "    # Create pixel indices for x and y coordinates  \n",
    "    pixel_index = np.arange(resolution_x * resolution_y)  \n",
    "    pixel_index_x = pixel_index % resolution_y  # X coordinate  \n",
    "    pixel_index_y = pixel_index // resolution_y  # Y coordinate  \n",
    "  \n",
    "    def get_inv_intrinsic(intrinsic:np.array) -> np.array:\n",
    "        \"\"\" Compute the inverse of the intrinsic camera matrix.\n",
    "\n",
    "        Args:\n",
    "            intrinsic (np.array): A 3x3 intrinsic camera matrix.  \n",
    "    \n",
    "        Returns:  \n",
    "            np.array: The inverse of the intrinsic camera matrix, which is also a 3x3 matrix.  \n",
    "        \"\"\"  \n",
    "        return np.array([[1 / intrinsic[0, 0], 0, -intrinsic[0, 2] / intrinsic[0, 0]],  # Inverse focal length and principal point x  \n",
    "                        [0, 1 / intrinsic[1, 1], -intrinsic[1, 2] / intrinsic[1, 1]],  # Inverse focal length and principal point y  \n",
    "                        [0, 0, 1]])  # Homogeneous coordinate for 2D to 3D transformation \n",
    "  \n",
    "    # Compute the inverse of the intrinsic matrix  \n",
    "    inv_intrinsic = get_inv_intrinsic(intrinsic)  \n",
    "  \n",
    "    # Create a homogeneous pixel coordinate matrix  \n",
    "    pixel_homo_t = np.stack([pixel_index_x, pixel_index_y, np.ones(resolution_x * resolution_y)])  \n",
    "  \n",
    "    # Convert pixel coordinates to camera coordinates  \n",
    "    camera_homo_t = inv_intrinsic @ pixel_homo_t  \n",
    "  \n",
    "    # Scale the camera coordinates by the depth values to obtain 3D points  \n",
    "    camera_pc_t = camera_homo_t * real_depth  \n",
    "  \n",
    "    # Prepare the point cloud data in homogeneous coordinates  \n",
    "    camera_pc_h_t = np.ones(shape=(camera_pc_t.shape[0] + 1, camera_pc_t.shape[1]), dtype=np.float32)  \n",
    "    camera_pc_h_t[:3, :] = camera_pc_t  # Set the first three rows to the camera points  \n",
    "  \n",
    "    # Create an Open3D PointCloud object  \n",
    "    camera_pc_o3d = o3d.geometry.PointCloud()  \n",
    "    camera_pc_o3d.points = o3d.utility.Vector3dVector(camera_pc_t.T)  # Set the points  \n",
    "    camera_pc_o3d.colors = o3d.utility.Vector3dVector(rgb_list)  # Set the colors  \n",
    "  \n",
    "    return camera_pc_o3d  # Return the point cloud object\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pattern Detect  \n",
    "  \n",
    "In this calibration process, we collect data for each hand-eye sample, which includes the board pose and the robot arm's end-effector pose. The DetectBoard function is utilized to detect the board pose from an RGB-depth image pair. \n",
    "\n",
    "This data is crucial for accurately establishing the geometric relationship between the camera and the robotic manipulator. \n",
    "\n",
    "- Pattern recognition includes two methods:\n",
    "\n",
    "  - `Concentric circle detection for Monocular Camera` uses a single camera to identify and analyze concentric circle patterns in images. This method is commonly employed in calibration tasks for robotic systems and computer vision applications. \n",
    "\n",
    "  - `Concentric circle detection for Binocular Camera` employs two cameras to capture images of the same concentric circle patterns from different viewpoints. This approach allows for depth perception and improved accuracy in determining the 3D positions of detected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Concentric Circle Detection for Monocular Camera\n",
    "\n",
    "\n",
    "#### Function: `rlia.calibration.ellipse_detect`\n",
    "This function detects concentric circles (ellipses) in the provided RGB images from a monocular camera setup. It utilizes the intrinsic and distortion parameters of both cameras to accurately identify the circle patterns.  \n",
    "  \n",
    "\n",
    "##### Parameters  \n",
    "- `rgb_t`: A list of RGB images provided as either [rows, cols] or [rows, cols, 3] uint8 format. This image is used to detect the concentric circles.\n",
    "- `depthO`: A [rows, cols] float tensor representing the depth map associated with the RGB image. This depth information aids in accurately reconstructing the 3D positions of the detected circles.\n",
    "- `k_t`: A [3, 3] double tensor representing the intrinsic camera matrix. This matrix includes parameters such as focal lengths and optical centers, which are essential for understanding how the camera captures the scene.\n",
    "- `d_t`: A [5] double tensor representing the distortion coefficients. These coefficients account for lens distortion, including radial and tangential distortions, and are crucial for correcting the captured image.\n",
    "\n",
    "##### Return\n",
    "- `report`: Structure returned by `ellipse_detect`.  \n",
    "  - `report.success`: Indicates the detect result status. \n",
    "  - `board_pose`: The pose of the center of the recognized concentric circles. The default position is np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries  \n",
    "import pickle  \n",
    "import numpy as np  \n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import matplotlib.pyplot as plt  \n",
    "import rlia  # Ensure you have the necessary rlia library  \n",
    "  \n",
    "# Load example data for hand-eye calibration  \n",
    "example_datas = rlia.data.HandeyeCalibrate()  # Instantiate the calibration data handler  \n",
    "example_path = example_datas.get_paths(\"single_marker\")[0]  # Get the path for the single marker data  \n",
    "  \n",
    "# Load the calibration data from the pickle file  \n",
    "with open(example_path, 'rb') as f:  \n",
    "    example_data = pickle.load(f)  \n",
    "  \n",
    "# Extract intrinsic camera parameters, distortion coefficients, RGB, and depth images  \n",
    "k = example_data[\"intrinsic\"]  # Intrinsic camera matrix  \n",
    "d = example_data[\"distortion\"]  # Distortion coefficients  \n",
    "rgb = example_data[\"rgb\"][0]  # First RGB image  \n",
    "depth = example_data[\"depth\"][0]  # First depth image  \n",
    "  \n",
    "# Convert parameters and images to Open3D tensors  \n",
    "k_t = o3c.Tensor(k, dtype=o3c.Dtype.Float64)  # Intrinsic matrix as tensor  \n",
    "d_t = o3c.Tensor(d, dtype=o3c.Dtype.Float64)  # Distortion coefficients as tensor  \n",
    "rgb_t = o3c.Tensor(rgb, dtype=o3c.Dtype.UInt8)  # RGB image as tensor  \n",
    "depth_t = o3c.Tensor(depth, dtype=o3c.Dtype.Float32)  # Depth image as tensor  \n",
    "  \n",
    "# Detect the concentric circles in the RGB and depth images  \n",
    "report = rlia.calibration.ellipse_detect(rgb_t, depth_t, k_t, d_t)  # Call the detection function  \n",
    "  \n",
    "# Check if the detection was successful  \n",
    "if report.success:  \n",
    "    print(\"Detected concentric circles\")  # Indicate successful detection  \n",
    "  \n",
    "    # Prepare the RGB image for visualization  \n",
    "    rgb_draw = rgb.copy()  # Create a copy of the RGB image for drawing  \n",
    "    if rgb_draw.ndim == 2:  # If the image is grayscale  \n",
    "        rgb_draw = np.repeat(rgb_draw[:, :, None], 3, axis=2)  # Convert to 3-channel RGB  \n",
    "  \n",
    "    # Convert the RGB image to tensor for drawing  \n",
    "    rgb_draw_t = o3c.Tensor(rgb_draw, dtype=o3c.Dtype.UInt8)  \n",
    "  \n",
    "    # Draw the detected pose on the image  \n",
    "    rgb_draw_t = rlia.calibration.DrawPoseOnImg(  \n",
    "        rgb_t=rgb_draw_t,  \n",
    "        k_t=k_t,  \n",
    "        pose_t=report.board_pose,  \n",
    "        axis_len=0.1  # Length of the axis to draw  \n",
    "    )  \n",
    "  \n",
    "    # Display the image with the drawn pose  \n",
    "    plt.imshow(rgb_draw_t.numpy())  \n",
    "    plt.axis('off')  # Turn off axis labels  \n",
    "    plt.show()  # Show the image  \n",
    "  \n",
    "    # Convert depth and RGB images to a point cloud  \n",
    "    pc_o3d = depth_rgb_to_pc(depth, rgb_draw, k)  \n",
    "    rlia.utility.draw([pc_o3d], backend='k3d', point_size=0.002) \n",
    "  \n",
    "    # Output the calibrated position of the concentric circles  \n",
    "    point_xyz = report.board_pose.numpy()  # Convert the pose tensor to a NumPy array  \n",
    "    print(\"Calibration result, position of concentric circles:\")  \n",
    "    print(point_xyz)  # Print the detected positions  \n",
    "else:  \n",
    "    print(\"Failed to detect concentric circles\")  # Indicate failure to detect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Concentric Circle Detection for Binocular Camera\n",
    "\n",
    "\n",
    "#### Function: `rlia.calibration.stereo_ellipse_detect`\n",
    "This function detects concentric circles (ellipses) in the provided RGB images from a binocular camera setup. It utilizes the intrinsic and distortion parameters of both cameras to accurately identify the circle patterns.  \n",
    "  \n",
    "##### Parameters  \n",
    "- `rgb_list`: A list of RGB images provided as either [rows, cols] or [rows, cols, 3] uint8 format. Each image should correspond to the left and right cameras in the binocular setup.  \n",
    "- `k_list`: A list of [3, 3] double camera intrinsic matrices for each camera. These matrices define the internal parameters of the cameras, including focal lengths and optical centers.  \n",
    "- `d_list`: A list of distortion coefficients for each camera, represented as [5] double matrices. The coefficients typically include:  \n",
    "  - k1, k2: Radial distortion coefficients.  \n",
    "  - p1, p2: Tangential distortion coefficients.  \n",
    "  - k3: Additional radial distortion coefficient.  \n",
    "- `R`: A [3, 3] double rotation matrix that describes the orientation of one camera relative to the other.  \n",
    "- `T`: A [3] double translation matrix that specifies the position of one camera relative to the other (unit: meters).\n",
    "\n",
    "##### Return\n",
    "- `report`: Structure returned by `stereo_ellipse_detect`.  \n",
    "  - `report.success`: Indicates the detect result status. \n",
    "  - `board_pose`: The pose of the center of the recognized concentric circles. The default position is np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries  \n",
    "import os  \n",
    "import json  \n",
    "import cv2  \n",
    "import numpy as np  \n",
    "import open3d as o3d  \n",
    "import open3d.core as o3c  \n",
    "import matplotlib.pyplot as plt  \n",
    "import rlia  # Ensure you have the necessary rlia library  \n",
    "np.set_printoptions(6, suppress=True)\n",
    "\n",
    "# Load example data for hand-eye calibration  \n",
    "example_datas = rlia.data.HandeyeCalibrate()  # Instantiate the calibration data handler  \n",
    "data_dir = example_datas.extract_dir  # Extract the data directory path  \n",
    "example_path = os.path.join(data_dir, \"stereo_params.json\")  # Define the path to the stereo parameters JSON file  \n",
    "  \n",
    "# Load the calibration data from the JSON file  \n",
    "with open(example_path, 'r') as f:  \n",
    "    example_data = json.load(f)  # Load the JSON content into a dictionary  \n",
    "  \n",
    "# Extract rotation matrix, translation vector, intrinsic parameters, and distortion coefficients  \n",
    "r = example_data[\"R_l_r\"]  # Rotation matrix from left to right camera  \n",
    "t = example_data[\"t_l_r\"]  # Translation vector from left to right camera  \n",
    "k_left = example_data[\"cam1_k\"]  # Intrinsic parameters of the left camera  \n",
    "k_right = example_data[\"cam2_k\"]  # Intrinsic parameters of the right camera  \n",
    "d_left = example_data[\"dist_1\"]  # Distortion coefficients for the left camera  \n",
    "d_right = example_data[\"dist_2\"]  # Distortion coefficients for the right camera  \n",
    "  \n",
    "# Load the stereo images from the data directory  \n",
    "img_left = cv2.imread(os.path.join(data_dir, \"stereo_left.bmp\"))  # Read the left stereo image  \n",
    "img_right = cv2.imread(os.path.join(data_dir, \"stereo_right.bmp\"))  # Read the right stereo image  \n",
    "  \n",
    "# Convert parameters and images to Open3D tensors  \n",
    "r_t = o3c.Tensor(r, dtype=o3c.Dtype.Float64)  # Rotation matrix as tensor  \n",
    "t_t = o3c.Tensor(t, dtype=o3c.Dtype.Float64).reshape(-1)  # Translation vector as tensor  \n",
    "k_left_t = o3c.Tensor(k_left, dtype=o3c.Dtype.Float64)  # Left camera intrinsic parameters as tensor  \n",
    "k_right_t = o3c.Tensor(k_right, dtype=o3c.Dtype.Float64)  # Right camera intrinsic parameters as tensor  \n",
    "d_left_t = o3c.Tensor(d_left, dtype=o3c.Dtype.Float64).reshape(-1)  # Left camera distortion coefficients as tensor  \n",
    "d_right_t = o3c.Tensor(d_right, dtype=o3c.Dtype.Float64).reshape(-1)  # Right camera distortion coefficients as tensor  \n",
    "img_left_t = o3c.Tensor(img_left, dtype=o3c.Dtype.UInt8)  # Left image as tensor  \n",
    "img_right_t = o3c.Tensor(img_right, dtype=o3c.Dtype.UInt8)  # Right image as tensor  \n",
    "  \n",
    "# Detect the concentric circles in the RGB images from both cameras  \n",
    "report = rlia.calibration.stereo_ellipse_detect(  \n",
    "    rgb_list=[img_left_t, img_right_t],  # List of RGB images  \n",
    "    k_list=[k_left_t, k_right_t],  # List of intrinsic parameters  \n",
    "    d_list=[d_left_t, d_right_t],  # List of distortion coefficients  \n",
    "    R=r_t,  # Rotation matrix  \n",
    "    T=t_t / 1000  # Translation vector, converted to meters (assuming input is in mm)  \n",
    ")  # Call the detection function  \n",
    "  \n",
    "# Check if the detection was successful  \n",
    "if report.success:  \n",
    "    print(\"Detected concentric circles\")  # Indicate successful detection  \n",
    "  \n",
    "    # Prepare the RGB image for visualization  \n",
    "    rgb_draw = img_left.copy()  # Create a copy of the left RGB image for drawing  \n",
    "    if rgb_draw.ndim == 2:  # If the image is grayscale  \n",
    "        rgb_draw = np.repeat(rgb_draw[:, :, None], 3, axis=2)  # Convert to 3-channel RGB  \n",
    "  \n",
    "    # Convert the RGB image to tensor for drawing  \n",
    "    rgb_draw_t = o3c.Tensor(rgb_draw, dtype=o3c.Dtype.UInt8)\n",
    "  \n",
    "    # Draw the detected pose on the image  \n",
    "    rgb_draw_t = rlia.calibration.DrawPoseOnImg(  \n",
    "        rgb_t=rgb_draw_t,  # Image tensor  \n",
    "        k_t=k_left_t,  # Left camera intrinsic parameters tensor  \n",
    "        pose_t=report.board_pose,  # Detected board pose  \n",
    "        axis_len=0.1  # Length of the axis to draw (in meters)  \n",
    "    )  \n",
    "  \n",
    "    # Display the image with the drawn pose  \n",
    "    plt.imshow(rgb_draw_t.numpy())  # Convert the tensor to NumPy array for display  \n",
    "    plt.axis('off')  # Turn off axis labels  \n",
    "    plt.show()  # Show the image  \n",
    "  \n",
    "    # Output the calibrated position of the concentric circles  \n",
    "    point_xyz = report.board_pose.numpy()  # Convert the pose tensor to a NumPy array  \n",
    "    print(\"Calibration result, position of concentric circles:\")  \n",
    "    print(point_xyz)  # Print the detected positions  \n",
    "else:  \n",
    "    print(\"Failed to detect concentric circles\")  # Indicate failure to detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handeye Calibrate\n",
    "\n",
    "In hand-eye calibration, the goal is to determine the relationship between a camera and a robotic manipulator. This is typically achieved using the AX = YB problem formulation, where:  \n",
    "- A represents the poses of the calibration board relative to the camera.  \n",
    "- B represents the poses of the robot's end effector relative to the robot base or a world coordinate system.  \n",
    "  \n",
    "##### 1. Concentric Circle Calibration for Monocular Camera\n",
    "- For monocular setups, the following functions are primarily used:  \n",
    "  - `rlia.calibration.ellipse_detect`: Detects concentric circles in the RGB and depth images.  \n",
    "  - `rlia.calibration.CalibrateAxyb`: Solves the AX = YB problem to calibrate the camera and end effector poses.  \n",
    "  \n",
    "##### 2. Concentric Circle Calibration for Binocular Camera\n",
    "- For binocular setups, the following functions are primarily employed:  \n",
    "  - `rlia.calibration.stereo_ellipse_detect`: Detects concentric circles in the RGB images from both cameras.  \n",
    "  - `rlia.calibration.CalibrateAxyb`: Solves the AX = YB problem similarly to the monocular approach.  \n",
    "\n",
    "### Function: `rlia.calibration.CalibrateAxyb`\n",
    "\n",
    "#### Parameters  \n",
    "- `pose_board_t`: List of [4, 4] o3c.Tensor. Representing the poses of the calibration board relative to the camera coordinate system. Each pose includes both position and orientation. \n",
    "- `pose_ee_t`: List of [4, 4] o3c.Tensor. Representing the poses of the robot's end effector relative to the robot base or a world coordinate system calibration board and a robot's end effector.\n",
    "- `is_eye_in_hand`: bool. \n",
    "  - If `True`, indicates the camera is mounted on the end effector (eye-in-hand configuration). \n",
    "  - If `False`, indicates the camera is fixed to the robot base (eye-to-hand configuration).\n",
    "\n",
    "#### Return\n",
    "- `report`: Structure returned by `CalibrateAxyb`.  \n",
    "  - `report.status`: Indicates the calibration result status. A value of 0 indicates accurate calibration; any other value indicates inaccurate results.  \n",
    "  - `report.pose_calibrate.numpy()`: The calibrated pose of the camera relative to the end effector or base, returned as a NumPy array.  \n",
    "  - `report.reproject_err`: The translation error when the hand-eye transformation is reprojected to the end effector. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 (`Monocular Camera`) Concentric Circle Handeye-Calibration Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries  \n",
    "import os  \n",
    "import json  \n",
    "import cv2  \n",
    "import numpy as np  \n",
    "import open3d as o3d  \n",
    "import open3d.core as o3c  \n",
    "import matplotlib.pyplot as plt  \n",
    "import rlia  # Ensure you have the necessary rlia library \n",
    "np.set_printoptions(6, suppress=True)\n",
    "\n",
    "example_datas = rlia.data.HandeyeCalibrate()  # hand eye Calibrate\n",
    "example_path = example_datas.get_paths(\"single_marker\")[0]\n",
    "\n",
    "with open(example_path, 'rb') as f:  \n",
    "    example_data = pickle.load(f) \n",
    "\n",
    "# Extract intrinsic and distortion parameters from the loaded data  \n",
    "intrinsic = o3c.Tensor(example_data[\"intrinsic\"], dtype=o3c.Dtype.Float64)  \n",
    "distortion = o3c.Tensor(example_data[\"distortion\"], dtype=o3c.Dtype.Float64)  \n",
    "is_eye_in_hand = example_data[\"isEyeInHand\"]  # Flag indicating if the camera is mounted on the end-effector\n",
    "\n",
    "# Initialize lists to store poses and transformations  \n",
    "board_pose_list = []  \n",
    "ee_pose_list = []  \n",
    "t_target_to_camera = []  \n",
    "t_robot_pose = []  \n",
    "t_target_cloud = [] \n",
    "\n",
    "# Get the number of RGB images available in the example data  \n",
    "num_samples = len(example_data[\"rgb\"]) \n",
    "\n",
    "# Iterate over each sample  \n",
    "for i in range(num_samples):  \n",
    "    # Convert RGB and depth images to tensors  \n",
    "    rgb = o3c.Tensor(example_data[\"rgb\"][i], dtype=o3c.Dtype.UInt8)  # RGB image tensor  \n",
    "    depth = o3c.Tensor(example_data[\"depth\"][i], dtype=o3c.Dtype.Float32)  # Depth image tensor  \n",
    "  \n",
    "    # Detect the board pose from the RGB and depth tensors  \n",
    "    detect_report = rlia.calibration.ellipse_detect(rgb, depth, intrinsic, distortion)  \n",
    "  \n",
    "    # Store the detected board pose  \n",
    "    board_pose_list.append(detect_report.board_pose)  \n",
    "  \n",
    "    # Convert the end-effector pose to a tensor and store it  \n",
    "    ee_pose_tensor = o3c.Tensor(example_data[\"eePose\"][i], dtype=o3c.Dtype.Float64)  \n",
    "    ee_pose_list.append(ee_pose_tensor)  \n",
    "  \n",
    "    # Store the robot pose and the target cloud position  \n",
    "    t_robot_pose.append(example_data[\"eePose\"][i])  # Store the robot pose  \n",
    "    t_target_cloud.append(detect_report.board_pose.numpy()[:, 3])  # Store the target cloud position  \n",
    "    t_target_to_camera.append(detect_report.board_pose.numpy())  # Store the transformation from target to camera  \n",
    "  \n",
    "# Perform calibration using the collected board and end-effector poses  \n",
    "calibrate_report = rlia.calibration.CalibrateAxyb(board_pose_list, ee_pose_list, is_eye_in_hand)  \n",
    "  \n",
    "# Print calibration results in a structured format  \n",
    "print(\"Calibration Results:\")  \n",
    "print(\"\\tCalibrated success:\\t\", calibrate_report.status == 0) \n",
    "print(\"\\teprojection Error:\\t\", calibrate_report.reproject_err)\n",
    "print(\"\\tCalibrated Pose:\\n\", calibrate_report.pose_calibrate.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 (`Binocular Camera`) Concentric Circle Handeye-Calibration Example \n",
    "\n",
    "- You can refer to (`Monocular Camera`) Concentric Circle Handeye-Calibration Example, and then change the recognition function from `rlia.calibration.ellipse_detect` to `rlia.calibration.stereo_ellipse_detect`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad949f78b9e735dad4b47980fe4d9ca98f621e7c9e020e33cb851e4b7d129260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
